# -*- coding: utf-8 -*-
"""
Created on Tue Jun 18 18:55:59 2019

@author: USER
"""

import pandas as pd
import numpy as np
from sklearn.feature_selection import SelectKBest,f_classif
from sklearn.feature_selection import chi2,f_regression,SelectPercentile
from sklearn.ensemble import ExtraTreesClassifier
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
from xgboost import XGBRegressor,XGBClassifier
from xgboost import plot_importance  
from matplotlib import pyplot 
from sklearn.tree import DecisionTreeClassifier 

def get_mae_XGB_Classifier(max_leaf_nodes, train_X, val_X, train_y, val_y):
    model = XGBClassifier(max_leaf_nodes=max_leaf_nodes, random_state=0)
    model.fit(train_X, train_y)
    preds_val = model.predict(val_X)
    mae = mean_absolute_error(val_y, preds_val)
    return(mae) 
    

def ModelSelectKBest(dataset):
    malware_feature = dataset.columns
    dataset = dataset.dropna(axis=0)
    X= dataset[malware_feature]  #independent columns
    y = dataset.Class    #target column i.e price range
    #apply SelectKBest class to extract top 10 best features
    bestfeatures = SelectKBest(f_classif,k=5)
    fit = bestfeatures.fit(X,y)
    dfscores = pd.DataFrame(-np.log10(fit.scores_))
    dfcolumns = pd.DataFrame(X.columns)
    #concat two dataframes for better visualization 
    featureScores = pd.concat([dfcolumns,dfscores],axis=1)
    featureScores.columns = ['Specs','Score']  #naming the dataframe columns
    print(featureScores.nlargest(20,'Score'))
    return featureScores.nlargest(20,'Score');
    
def DecisionTreeClassifierImportant(dataset):
    malware_feature = dataset.columns
    dataset = dataset.dropna(axis=0)
    malware_feature=malware_feature.drop("Class")
    
    X = dataset[malware_feature]  #independent columns
    y = dataset.Class
    model = DecisionTreeClassifier()
    model.fit(X,y)
    feat_importances = pd.Series(model.feature_importances_, index=X.columns)
    feat_importances.nlargest(20).plot(kind='barh')
    feat_importances = pd.Series(model.feature_importances_, index=X.columns)
    feat_importances.nlargest(20).plot(kind='barh')
    plt.show()
    pyplot.show()
    return feat_importances.nlargest(20).index
    
def XGBClassifierMalwareImportantFeature(dataset):
    malware_feature = dataset.columns
    dataset = dataset.dropna(axis=0)
    malware_feature=malware_feature.drop("Class")
    X = dataset[malware_feature]  #independent columns
    y = dataset.Class
    train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)
    candidate_max_leaf_nodes = [5, 25, 50, 100, 250, 500,1000]
    for max_l in candidate_max_leaf_nodes :
         get_mae_XGB_Classifier(max_l, train_X, val_X, train_y, val_y)
    scores = {leaf_size: get_mae_XGB_Classifier(leaf_size, train_X, val_X, train_y, val_y) for leaf_size in candidate_max_leaf_nodes}
    best_tree_size = min(scores, key=scores.get)
    print('best tree size: ',best_tree_size)
    
    XGBClassifierMalware=XGBClassifier(learning_rate=0.1,max_leaf_nodes=best_tree_size,n_estimators=100)
    XGBClassifierMalware.fit(train_X, train_y, 
                 early_stopping_rounds=5, 
                 eval_set=[(val_X, val_y)], 
                 verbose=False)

    features_W = pd.Series(XGBClassifierMalware.get_booster().get_score(importance_type='weight'), index=X.columns)
    features_W.sort_values(axis=0, ascending=False).nlargest(25).plot(kind='barh').set_title('XGBClassifierMalware_weight')
    plt.show()
    
    feat_importances = pd.Series(XGBClassifierMalware.feature_importances_, index=X.columns)
    feat_importances.sort_values(axis=0, ascending=False)
    print(feat_importances.values)
    print('nico','\r\n')
    
    print(feat_importances[feat_importances.values > 0.001])
    best=feat_importances[feat_importances.values > 0.001]
    feat_importances.nlargest(20).plot(kind='barh').set_title('XGBClassifierMalware')
    plt.show()
    
    plot_importance(XGBClassifierMalware,max_num_features=22)
    pyplot.show()
    return best.index

def XGBMalwareModelRegressorImportantFeature(dataset):
    malware_feature = dataset.columns
    dataset = dataset.dropna(axis=0)
    malware_feature=malware_feature.drop("Class")
    X = dataset[malware_feature]  #independent columns
    y = dataset.Class
    train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)
    XGBMalwareModel =   XGBRegressor(n_estimators=1000, learning_rate=0.7)
    XGBMalwareModel.fit(train_X, train_y, 
                 early_stopping_rounds=5, 
                 eval_set=[(val_X, val_y)], 
                 verbose=False)
    feat_importances = pd.Series(XGBMalwareModel.feature_importances_, index=X.columns)
    feat_importances.nlargest(20).plot(kind='barh').set_title('XGBMalwareModelRegressor important features')
    plt.show()
    return feat_importances.nlargest(20).index


from sklearn.ensemble import RandomForestClassifier 
def RFImportant(dataset):
    malware_feature = dataset.columns
    dataset = dataset.dropna(axis=0)
    malware_feature=malware_feature.drop("Class")
    
    X = dataset[malware_feature]  #independent columns
    y = dataset.Class
    model = RandomForestClassifier()
    model.fit(X,y)
    feat_importances = pd.Series(model.feature_importances_, index=X.columns)
    feat_importances.nlargest(20).plot(kind='barh').set_title('RF important features')
    plt.show()
    pyplot.show()
    return feat_importances.nlargest(20).index